{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Innovation Lab\n",
    "\n",
    "## Introduction to AI and the data science cycle\n",
    "\n",
    "Chidi Nweke\n",
    "\n",
    "Academic year 2023-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùó This notebook contains images, if you're viewing it from Colab you will likely not have the full experience, sorry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aims of this session are to introduce AI, machine learning and data science from a practical point of view. \n",
    "\n",
    "1. We will start off with some common definitions.\n",
    "2. Afterwards we will give an overview of the most common technologies used in AI. \n",
    "3. We will briefly touch on CRISP-DM, the data science cycle.\n",
    "4. We then proceed with a case study.\n",
    "5. Finally we look at the challenges of using AI-based systems in real world settings.\n",
    "   \n",
    "The goal of this session is that you participate and run the code cells. You do not need to understand the inner workings of these models but rather that they require simple code and have APIs that are accessible to everyone, not just AI experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial intelligence vs machine learning vs data science\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data science\n",
    "\n",
    "Data science is all about using data to make informed decisions.\n",
    "\n",
    "1. grocery stores use data to decide how much stock they need to have. \n",
    "2. De Lijn uses it to decide what bus lines have too many or little people.\n",
    "3. Google uses it to decide what search results to show you.\n",
    "\n",
    "#### ‚ùì Any other examples?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial intelligence (AI)\n",
    "\n",
    "AI's goal is to make \"machines\" that can make intelligent decisions by themselves.\n",
    "\n",
    "1. Some grocery chains use it to automatically place orders for items.\n",
    "2. Stockfish is an AI chess engine made by a collaboration of programmers and chess experts.\n",
    "3. Alpha zero is a chess AI that learnt to play chess by playing against itself.\n",
    "4. Google uses it to decide what search results to show you.\n",
    "5. You use hopefully don't use chatGPT to make part of your homework.\n",
    "\n",
    "\n",
    "#### ‚ùì Is there overlap between data science and AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have overlap but they're different things. It is possible to do data science without AI. For example, plotting data and doing statistics are an example of something that is data science but not AI.\n",
    "\n",
    "It is also possible to do AI without data. Stockfish is a good example of that, it's a rule-based AI system. Experts and programmers came together to define the values to positions on the chessboard and the algorithm looks for the most optimal moves to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning (ML)\n",
    "\n",
    "Machine learning is a subset of AI where the AI algorithm \"trained\" by feeding it data. This definition likely sounds very vague. Instead of sticking with the theory we'll look at practical examples of AI's most promising subset: machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are several technologies and programming languages that can be used for data science and AI. The image below is an overview of canonical packages used in this domain. The practical part of the course and the assignments will be done through jupyter notebooks. Jupyter stands for Julia, Python and R, the three main programming languages that are used. In this session we will focus on Python and a subset of relevant packages used in data science and AI. \n",
    "\n",
    "<center><img src=\"images\\9rnujgv697bmk2jq3yip.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM and the data science process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/CRISP-DM.png\" style=\"background-color:white; max-width:50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRISP-DM stands for Cross Industry Standard Process for Data Mining. Essentially, it's a way of working you can use across various industry for data mining (data science) work.\n",
    "\n",
    "The key idea behind CRISP-DM is that you:\n",
    "\n",
    "1. First start with understanding the business \"What problem are we trying to solve\".\n",
    "2. Afterwards you look at the data and understand what you're working with.\n",
    "3. Data preparation is the next phase, here is where you use your SQL and Python skills to transform the data into something you can work with.\n",
    "4. Modelling is where you create your ML-model.\n",
    "5. Afterwards you can evaluate and deploy your solution if it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Before we start what do these images have in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px;\">\n",
    "    <img src=\"images/titanic.jpg\" style=\"max-width: 100%; height: auto;\">\n",
    "    <img src=\"images/titanic_film.jpg\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our case study: The Titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late in the evening of the 14th of April 1912 a ship on its way from Southampton to London called the Titanic collided with an Iceberg and tragically crashed.  You, being interested in data & AI, realise you can use this information to figure out who is likely to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have access to the following variables:\n",
    "\n",
    "<blockquote> \n",
    "\n",
    "survival - Survival (0 = No; 1 = Yes)\n",
    "\n",
    "class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "\n",
    "sibsp - Number of Siblings/Spouses Aboard\n",
    "\n",
    "parch - Number of Parents/Children Aboard\n",
    "\n",
    "ticket - Ticket Number\n",
    "\n",
    "fare - Passenger Fare\n",
    "\n",
    "cabin - Cabin\n",
    "\n",
    "embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "</blockquote> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \n",
    "\n",
    "#### ‚ùì How would you solve this with or without AI?\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case our goal is simple: we want to know who does and doesn't survive the Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üíªüìä We will tackle all of them together using Python's pandas package. Typically it is imported as ```import pandas as pd```. \n",
    "\n",
    "Pandas is Python's most popular package for working with data frames. These are likely Excel spreadsheets you can manipulate using Python.\n",
    "\n",
    "**Pandas' syntax is very similar to that of SQL.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Common convention\n",
    "\n",
    "titanic_df = pd.read_csv('data/train.csv')\n",
    "titanic_df.head() # Top 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí° This phase is all about exploring and asking the right questions to understand the data. The more experience you get with data & AI the easier it becomes to formulate these. In the course we will give you a number of standard questions you can ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \n",
    "\n",
    "**Q1:** How much data do I have?\n",
    "\n",
    "**Q2:** There are a few missing values in the Cabin column. How big of a problem is this?  (Typically machine learning algorithms can not deal with null values)\n",
    "\n",
    "**Q3:** How many people survived in this dataset?\n",
    "\n",
    "**Q4:** Are the tickets names standardised? What do they mean? \n",
    "\n",
    "</blockquote> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data do I have\n",
    "len(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I have missing data?\n",
    "\n",
    "titanic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí° Machine learning models don't like missing values.\n",
    "\n",
    "We can't replace a missing Age by 0. If we did that the algorithm would think the person is a baby and maybe the chance to survive is linked to your age. In our case we'll replace them with the average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many people survived \n",
    "titanic_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df[\"Ticket\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \n",
    "\n",
    "#### ‚ùìBefore we start analyzing the data: do you think all of these variables are relevant to predict who does or does not survive? Which would you remove?\n",
    "\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since everyone has a different ticket, it doesn't tell us anything about who survives and who doesn't.\n",
    "2. Age and Cabin have missing values so for this session we will remove them.\n",
    "3. Everyone has a unique id so that doesn't tell us anything about who will survive and who doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üíªüìä To quickly summarize the data Pandas has a ```describe()``` method. You can also use Pandas in a way that is close to SQL for example ```dataframe.groupby(\"yourGroup\").max()``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupby([\"Survived\", \"Pclass\"]).size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupby([\"Survived\"]).mean()[\"Age\"] # [\"Age\"] means that only the mean of the Age column is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  To gain more understanding in the data and/or to present your preliminary results to people that do not have a background in data & AI visualisations can help. We will use ```matplotlib``` and ```seaborn``` import them respectively as ```import matplotlib.pyplot as plt``` and ```import seaborn as sns```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, don't worry about the syntax of the plot. We will cover that in later lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(titanic_df[[\"Survived\", \"Pclass\",\"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]], hue=\"Survived\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°üíªüìä  Notice how there are two square brackets around titanic_df. You can access a column with ```dataFrame[\"columnName\"]```. If you want to access multiple columns you need to pass in a list like: ```dataFrame[ListOfColumns].```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° This graph isn't the best. It contains *too much* information. If you need to explain a plot to the reader it's likely too complicated. The goal is to show you how easy it is to make powerful plots using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \n",
    "\n",
    "##### ‚ùì What insights can we get from the pairplot?\n",
    "\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more insights you could get from looking at this plot. I recommend you to do this later on and see what more information you could get out of it.\n",
    "\n",
    "<blockquote> \n",
    "\n",
    "**Insight 1:** The largest group of survivors were in first class.\n",
    "\n",
    "**Insight 2:** People with a higher fare seem to survive at a higher rate.\n",
    "\n",
    "**Insight 3:** Having a large number of siblings and parents aboard (4 +) lowers the survival rate.\n",
    "\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would typically clean our data, solve the missing values and so on. Since the Titanic case is very simple we luckily have no data preparation, we can immediately go to modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíªü§ñ ```Sci-kit learn``` is the main machine learning package we will be using. It contains most popular algorithms together with other things from the wider data science toolbox such as model evaluation and data preparation. A key feature of sci-kit learn are Pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üíªü§ñ The ```Pipeline``` class allows you to chain multiple steps after each other. This enables you to carry out transformations of your data (data preparation step in CRISP-DM) and modeling at the same time. Another advantage is that this enables you to deploy, monitor and version control your machine learning models in a correct way. Time permitting, We will discuss this at more length in the end of the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Machine learning models can only take numeric input.**\n",
    "\n",
    "We need to transform the textual variables to onehot encoded variables. The image below is an intuitive explanation\n",
    "\n",
    "<center> <img src=\"images\\1_ggtP4a5YaRx6l09KQaYOnw.png\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Machine learning models prefer that each variable is approximately between 0 and 1. If they aren't (like Age) you need to convert them.** \n",
    "\n",
    "<center> <img src=\"images\\standard scaling.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Finally, we impute (fill in the blanks) of the missing values with their average.**\n",
    "\n",
    "<center> <img src=\"images\\mean imputation.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \n",
    "\n",
    "##### ‚ùì What techniques would you apply to what variables? \n",
    "\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_and_scale = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (impute_and_scale, [\"Age\", \"Fare\"]),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"Sex\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\"]),\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things are happening here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `make_pipeline` lets us apply 2 things after each other.\n",
    "   1.  First a `SimpleImputer` replaces the missing values by the average.\n",
    "   2.  Then a standardScaler scales the data to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `make_column_transformer` applies the first pipeline `input_and_scale` to the age and Fare column and applies `OneHotEncoder` to other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The ```OneHotEncoder``` applies the transformation as discussed above. Each category is turned into its own column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ```remainder='drop'``` means that the variables that are not selected by the column transformers get removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.fit_transform(titanic_df).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in a format we can give to a machine learning model we can apply a few techniques. \n",
    "\n",
    "We will evaluate the performance using `cross_val_score`. It divides the data 5 times and makes 5 machine learning models. It then checks how accurate it is on 1/5th of the data that wasn't used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = titanic_df.drop(\"Survived\", axis=1)\n",
    "y = titanic_df[\"Survived\"]\n",
    "logisticReg = make_pipeline(preprocessing, LogisticRegressionCV(max_iter=10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logisticReg, X, y) # 80 % correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote> \n",
    "\n",
    "#####  Two questions\n",
    "\n",
    "1. ‚ùì Is this a good score?\n",
    "2. ‚ùì Can we trust the algorithm? How do we know how it decides?\n",
    "\n",
    "</blockquote> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare performance we need to compare to so-called baselines. A common baseline is predicting the most frequent class (in this case 0, not surviving). ```Sci-kit learn``` supports this functionality with ```DummyClassifier``` and ```DummyRegressor```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "cross_val_score(dummy, X, y) # 80 % correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced, set-up code\n",
    "logisticReg.fit(X, y)\n",
    "\n",
    "ohe_feature_names = preprocessing.named_transformers_['onehotencoder'].get_feature_names_out([\"Sex\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\"])\n",
    "original_features = [\"Age\", \"Fare\"]\n",
    "variables = original_features + list(ohe_feature_names)\n",
    "importance = logisticReg[-1].coef_.flatten()\n",
    "\n",
    "coef_df = pd.DataFrame({\"variables\": variables, \"importance\": importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20,8))\n",
    "sns.barplot(data=coef_df, x=\"variables\", y=\"importance\")\n",
    "plt.xticks(rotation=45) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ‚ùì What information can you derive from this? Remember: above 0.5 is surviving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes a lot of sense. Those that watched the movie might remember that women and children were allowed on the rubber boats first which means they were a lot more likely to survive. People in the first class survived at a higher rate than in the third class as well. Finally, The less siblings you and children you had aboard the more chance you had to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to visualise the results of a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = make_pipeline(preprocessing, DecisionTreeClassifier(max_depth=4))\n",
    "cross_val_score(decisionTree, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree.fit(X, y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(12,12), dpi=1000)\n",
    "plot_tree(decisionTree[-1], feature_names=variables);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Decision trees are like giving your data to AI and getting IF-then rules automatically.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we've speed tracked both the evaluation and modelling section. In reality they're a lot more elaborate than this, however for the sake of this workshop. We will keep it at this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the story ended a lot previously, data scientists would build machine learning models, find interesting results and present them. Deployment is becoming more and more important. Dall-E and ChatGPT are not only success stories of machine learning being done in practice, but also of companies succeeding in deploying and scaling very large models to end users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLops\n",
    "\n",
    "For a successful deployment strategy you need to treat machine learning models like you would software and have an integrated strategy. Open source tools like MLflow offer this, they enable you to register metrics, plots and more centrally, they let you version control your models and even have a service that allows you to turn models into a RESTful API.\n",
    "\n",
    "<center><img src=\"https://www.databricks.com/sites/default/files/2020/04/databricks-adds-access-control-to-mlflow-model-registry_01.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orchestration and data engineering\n",
    "\n",
    "A second element is the orchestration of this process, right now we had a static dataset but in reality data is coming on and out continuously. Tools such as Dagster.io and Airflow let you organize all of this work.\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px;\">\n",
    "    <img src=\"https://preview.redd.it/v3zqpswvmgi81.png?width=1547&format=png&auto=webp&s=aa7bfb8ce9887953b225911a39b4e2645948bfb0\" style=\"max-width: 100%; height: auto;\">\n",
    "    <img src=\"https://developer.ibm.com/developer/default/articles/create-ai-pipelines-using-elyra-and-apache-airflow/images/pipeline-in-pipeline-editor.png\" style=\"max-width: 100%; height: auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's simplify that further:\n",
    "\n",
    "#### Scaling the backend\n",
    "\n",
    "Today, many web developers use `node.js` or Python's `asyncio`. These tools are good for tasks where the system spends a lot of time waiting , like waiting for data from a database (IO bound). During this wait, the system can do other things.\n",
    "\n",
    "<center><img src=\"https://www.devtip.co/content/images/2023/01/BTm1H.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Threads: Think of threads as smaller parts of a program. You can run the machine learning in one thread and the rest of the program in another. For node.js, you can use something called worker threads.\n",
    "\n",
    "2. Task Queues: Use tools like Celery. With this, you can give the machine learning task to another computer or program. When it's done, it sends back the results.\n",
    "\n",
    "3. Microservices: You can have the machine learning run by itself, separate from the main program. The main program can ask this separate service for results when needed.\n",
    "\n",
    "4. Special Platforms: There are special tools like TensorFlow Serving that are made to run machine learning tasks quickly.\n",
    "\n",
    "Different tasks might need different solutions. It's important to pick the one that works best for your needs. Here is where it's important to have the expertise of not only data scientists but also people with your background"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "701e02254c641c504e98600f30db52618525bf75a7cd858e6fd9b809d91e9f25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
